

\chapter{Códigos BCH}

% 4.5 Mininum distance of cyclic codes (intro cota BCH)

En esta sección vamos a estudiar los códigos \textacr{BCH}, un tipo de códigos cíclicos que permiten ser diseñados con una capacidad de corrección concreta.
Como ya sabemos, para cualquier tipo de código es importante determinar la distancia mínima si queremos determinar su capacidad de corrección de errores.
A este respecto es útil disponer de cotas en la distancia mínima, especialmente cotas inferiores, pues son las que maximizan la capacidad de corrección.
Existen varias cotas conocidas para la distancia mínima de un código cíclico, pero nos vamos a centrar en la llamada \textit{cota de Bose-Ray-Chaudhuri-Hocquenghem}, usualmente abreviada como \textit{cota \textacr{BCH}}.
Esta cota es esencial para comprender la definición de los códigos \textacr{BCH} que estudiamos en esta sección.
La cota \textacr{BCH} va a depender de los ceros del código, concretamente en la posibilidad de encontrar cadenas de ceros «consecutivos».
La fuente principal de este capítulo ha sido \parencite{huffman_fundamentals_2003}.

\section{Construcción de códigos BCH}

En lo que sigue vamos a considerar un código cíclico \(\mathcal C\)  de longitud \(n\) sobre \(\mathbb F_q\) y \(\alpha\) una enésima raíz primitiva de la unidad en \(\mathbb F_{q^t}\), donde \(t = \operatorname{ord}_n(q)\).
Recordemos que \(T\) es un conjunto característico de \(\mathcal C\) siempre y cuando los ceros de \(\mathcal C\) sean \(\{\alpha^{i} : i \in T\}\).
Por tanto \(T\) ha de ser una unión de clases \(q\)-ciclotómicas módulo \(n\).
Decimos que \(T\) contiene un conjunto de \(s\) \textit{elementos consecutivos} si existe un conjunto \(\{b, b + 1, \dots, b + s - 1\}\) de \(s\) enteros consecutivos tal que
\[
  \{b, b + 1, \dots, b + s - 1\} \bmod n = S \subseteq T.
\]

Antes de proceder con la cota \textacr{BCH} vamos a enunciar un lema —que será utilizado en la demostración de dicha cota— sobre el determinante de una matriz de Vandermonde.
Sean \(\alpha_1, \dots, \alpha_s\) elementos de un cuerpo \(\mathbb F\).
La matriz de tamaño \(s \times s\) dada por \(V = (v_{i, j})\), donde \(v_{i,j} = \alpha_{j}^{i-1}\) se denomina \textit{matriz de Vandermonde}.
Observamos que la transpuesta de una matriz de Vandermonde es otra matriz de Vandermonde.

\begin{lemma}
  \label{lem:vandermonde}
  El determinante de una matriz de Vandermonde \(V\) viene dado por \(\operatorname{det}V = \prod_{1 \leq i < j \leq s}(\alpha_j - \alpha_i)\).
  En particular, \(V\) es no singular si los elementos \(\alpha_1, \dots, \alpha_s\) son todos diferentes dos a dos.
\end{lemma}

Estamos ya en condiciones de presentar y demostrar el teorema de la cota \textacr{BCH}.

\begin{theorem}[cota bch]
  Sea \(\mathcal C\) un código cíclico de longitud \(n\) sobre \(\mathbb F_q\) con conjunto característico \(T\).
  Supongamos que \(\mathcal C\) tiene peso mínimo \(d\).
  Asumamos que \(T\) contiene \(\delta - 1\) elementos consecutivos para algún entero \(\delta\).
  Entonces, \(d \geq \delta\).
\end{theorem}

\begin{proof}
  Asumimos que el código \(\mathcal C\) tiene ceros que incluyen
  \[
    \alpha^b, \alpha^{b+1}, \dots, \alpha^{b + \delta - 2}.
  \]
  Sea \(c(x)\) una palabra código de \(\mathcal C\) de peso \(w\) de la forma
  \[
    c(c) = \sum_{j = 1}^w c_{i_j}x^{i_j}.
  \]
  Vamos a proceder por reducción al absurdo.
  Supongamos que \(w \leq \delta\).
  Como \(c(\alpha^i) = 0\) para \(b \leq i \leq b + \delta - 2\), \(M \mathbf{u}^T = \mathbf{0}\), donde
  \[
    M = \begin{pmatrix}
      \alpha^{i_1b} & \alpha^{i_2b} & \dots & \alpha^{i_wb}\\
      \alpha^{i_1(b+1)} & \alpha^{i_2(b+1)} & \dots & \alpha^{i_w(b+1)}\\
        & & \vdots & \\
      \alpha^{i_1(b+w-1)} & \alpha^{i_2(b+w-1)} & \dots & \alpha^{i_w(b+w-1)}\\
    \end{pmatrix}
  \]
  y \(\mathbf{u} = c_{i_1}c_{i_2}\dotsc_{i_w}\).
  Como \(\mathbf{u} \neq \mathbf{0}\) la matriz \(M\) es singular, y por tanto \(\det M = 0\).
  Pero \(\det M = \alpha^{(i_1 + i_2 + \dots + i_w)b}\det V\), donde \(V\) es la matriz de Vandermonde
  \[
    V = \begin{pmatrix}
      1 & 1 & \dots & 1 \\
      \alpha^{i_1} & \alpha^{i_2} & \dots & \alpha^{i_w} \\
       &  & \vdots & \\
      \alpha^{i_1(w - 1)} & \alpha^{i_2(w - 1)} & \dots & \alpha^{i_w(w - 1)} \\
    \end{pmatrix}.
  \]
  Como los \(\alpha^{i_j}\) son todos distintos dos a dos, \(\det V \neq 0\) por el lema \ref{lem:vandermonde}, lo que contradice que \(\det M = 0\).
\end{proof}

Los códigos \textacr{BCH} son códigos cíclicos diseñados para aprovechar la cota \textacr{BCH}.
Nos gustaría poder construir un código cíclico \(\mathcal C\) de longitud \(n\) sobre \(\mathbb F_q\) que tenga a la vez un peso mínimo grande y una dimensión grande.
Tener un peso mínimo grande, basándonos en la cota \textacr{BCH}, se puede conseguir escogiendo un conjunto característico para \(\mathcal C\) que tenga un gran número de elementos consecutivos.

Como la dimensión de \(\mathcal C\) es \(n - |T|\) por el teorema \ref{th:cicl-cto-caracteristico}, nos gustaría que \(|T|\) fuese tan pequeño como sea posible.
Por tanto, si quisiésemos que \(\mathcal C\) tenga distancia mínima de al menos \(\delta\), podemos escoger un conjunto característico tan pequeño como sea posible que sea una unión de clases \(q\)-ciclotómicas con \(\delta - 1\) elementos consecutivos.

Sea \(\delta\) un entero tal que \(2 \leq \delta \leq n\). Un \textit{código \textacr{BCH}} \(\mathcal C\) sobre \(\mathbb F_q\) de longitud \(n\) y \textit{distancia mínima prevista} \(\delta\) es un código cíclico con conjunto característico
\begin{equation}
  \label{eq:bch-conjunto-caracteristico}
  T = C_b \cup C_{b+1} \cup \dots \cup C_{b + \delta - 2},
\end{equation}
donde \(C_i\) es la clase \(q\)-ciclotómica módulo \(n\) que contiene a \(i\).
Por la cota \textacr{BCH} este código tiene distancia mínima prevista al menos \(\delta\).

\begin{theorem}
  Un código \textacr{BCH} de distancia mínima prevista \(\delta\) tiene peso mínimo de al menos \(\delta\).
\end{theorem}

\begin{proof}
  El conjunto característico \ref{eq:bch-conjunto-caracteristico} tiene al menos \(\delta - 1\) elementos.
  El resultado se deduce de la cota \textacr{BCH}.
\end{proof}

Al variar el valor de \(b\) obtenemos distintos códigos con distancias mínimas y dimensiones diferentes.
Cuando \(b = 1\) el código \(\mathcal C\) se dice que es un código \textacr{BCH} \textit{en sentido estricto}.
Como con cualquier código cíclico, si \(n = q^t - 1\) entonces \(\mathcal C\) es un código \textacr{BCH} \textit{primitivo}.
En la sección siguiente vamos a estudiar un algoritmo de decodificación que permite aprovechar las ventajas de los códigos \textacr{BCH}.

\section{Códigos Reed-Solomon}

Vamos a describir brevemente los códigos Reed-Solomon, que abreviaremos como códigos RS, pues aludiremos a ellos cuando hablemos de códigos cíclicos sesgados.
Son una subfamilia de los códigos BCH que acabamos de definir.

\begin{definition}
  Un código RS sobre \(\mathbb F_q\) es un código BCH de longitud \(n = q - 1\).
\end{definition}

Veamos un par de propiedades importantes.

\begin{theorem}
  Sea \(\mathcal C\) un código RS sobre \(\mathbb F_q\) de longitud \(n = q - 1\) y distancia mínima prevista \(\delta\).
  Entonces:
  \begin{enumerate}
    \item El código \(\mathcal C\) tiene conjunto característico \(T = \{b, b + 1, \dots, b + \delta - 2\}\) para algún entero \(b\).
    \item El código \(\mathcal C\) tiene distancia mínima \(d = \delta\) y dimensión \(k = n - d + 1\).
  \end{enumerate}
\end{theorem}

\begin{proof}
  %Comenzamos observando que \(\operatorname{ord}_n(q) = 1\), por lo que todos los factores irreducibles de \(x^n - 1\) tienen grado \(1\) y las clases \(q\)-ciclotómicas módulo \(n\) tienen tamaño \(1\).
  Veamos la demostración por apartados.
  \begin{enumerate}
    \item Como es un código BCH de distancia mínima prevista \(\delta\) su conjutno característico \(T\) tiene que tener tamaño \(\delta - 1\) y en consecuencia es \(T = \{b, b + 1, \dots, b + \delta - 2\}\).
    \item La distancia es obvia, pues es un código BCH y la dimensión proviene del teorema \ref{th:cicl-cto-caracteristico}, pues \(n - |T| = n - \delta + 1\).
  \end{enumerate}
\end{proof}


\section{Algoritmo de Peterson-Gorenstein-Zierler}

El algoritmo de Peterson-Gorenstein-Zierler —de ahora en adelante, algoritmo \textacr{PGZ}— es un algoritmo de decodificación de códigos \textacr{BCH} que permite corregir hasta \(t = \lfloor (\delta - 1)/2 \rfloor\) errores.
Fue desarrollado originalmente en 1960 por Peterson \parencite{peterson_encoding_1960} para decodificar códigos \textacr{BCH} binarios, pero fue generalizado poco después por Gorenstein y Zierler para códigos no binarios \parencite{gorenstein_class_1961}.

Como en cualquier otro método de decodificación el objetivo es obtener el mensaje original \(c(x)\) a partir de un mensaje recibido \(y(x)\), para lo que hay que hallar primero los errores \(e(x)\) que se han producido en la transmisión, de forma que \(c(x) = y(x) - e(x)\).
El vector de errores ha de tener peso \(v \leq t\), ya que no podemos corregir más errores de los que el código permite.
Vamos a considerar que los errores se han producido en coordenadas desconocidas \(k_1, k_2, \dots, k_v\), de forma que el vector de errores lo podemos expresar como
\[
  e(x) = e_{k_1}x^{k_1} + e_{k_2}x^{k_2} + \dots + e_{k_v}x^{k_v}.
\]
Como nuestro objetivo es determinar \(e(x)\) tenemos que hallar: \begin{itemize}
  \item las \textit{coordenadas de error} \(k_j\)\,;
  \item las \textit{magnitudes de error} \(e_{k_j}\).
\end{itemize}

Vamos a estudiar a continuación el desarrollo teórico y la justificación del funcionamiento del algoritmo para después dar una versión del mismo esquematizada en pseudocódigo.
El comienzo de este método es similar al del descrito en la sección \ref{subsec:codificacion-descodificacion}, solo que en lugar de utilizar la propiedad de la matriz de paridad allí descrita utilizaremos la propiedad análoga de que, por el teorema \ref{th:cicl-cto-caracteristico}, un elemento \(c(x) \in \mathcal C\) si y solo si \(c(\alpha^i) = 0\) para todo \(i \in T\).
En nuestro caso particular, dado que \(t = \lfloor (\delta - 1)/2 \rfloor\) y \(T\) contiene a \(\{1, 2, \dots, \delta - 1\}\), se tiene que
\[
  y(\alpha^i) = c(\alpha^i) + e(\alpha^i) = e(\alpha^i)
\]
para todo \(1 \leq i \leq 2t\).
Estas ecuaciones van a ser fundamentales para encontrar el error \(e(x)\).
En este caso llamaremos \textit{síndrome} \(s_i\) de \(y(x)\) al elemento de \(\mathbb F_{q}^m\) dado por \(s_i = y(\alpha^i)\).
El primer paso del algoritmo es encontrar los síndromes para todo \(1 \leq i \leq 2t\).
Estos síndromes nos conducen a un sistema de ecuaciones en el que se encuentran las coordenadas de error \(k_j\) y las magnitudes de error \(e_{k_j}\).
Desarrollando lo anterior podemos expresar los síndromes como
\begin{equation}
  \label{eq:sindromes}
  s_i = y(\alpha^i) = \sum_{j = 1}^v e_{k_j}(\alpha^i)^{k_j}
   = \sum_{j = 1}^v e_{k_j}(\alpha^{k_j})^i
\end{equation}
para todo \(1 \leq i \leq 2t\).
A fin de simplificar la notación, para \(1 \leq j \leq v\) definimos: \begin{itemize}[label={—}, noitemsep, leftmargin=*]
  \item \(E_j = e_{k_j}\), que llamaremos \textit{magnitud de error en la coordenada} \(k_j\), y
  \item \(X_j = \alpha^{k_j}\), que llamaremos \textit{número de coordenada de error correspondiente a la coordenada} \(k_j\).
\end{itemize}
Observamos que al conocer \(X_j\) conocemos de forma unívoca la coordenada de error \(k_j\), ya que si \(\alpha^i = \alpha^k\) para \(i\) y \(k\) entre \(0\) y \(n-1\), entonces \(i = k\).
Con la notación que hemos descrito la igualdad (\ref{eq:sindromes}) la podemos escribir como \begin{equation}
  \label{eq:sindromes-alt}
  S_i = \sum_{j = 1}^v E_jX_{j}^i, \quad\text{para } 1 \leq i \leq 2t,
\end{equation}
lo que nos conduce al sistema de ecuaciones: \begin{equation}
  \begin{cases}
    S_1 = E_1X_1 + E_2X_2 + \dots + E_vX_v,\\
    S_2 = E_1X_1^2 + E_2X_2^2 + \dots + E_vX_v^2,\\
    S_3 = E_1X_1^3 + E_2X_2^3 + \dots + E_vX_v^3,\\
    \quad\;\vdots\\
    S_{2t} = E_1X_1^{2t} + E_2X_2^{2t} + \dots + E_vX_v^{2t}.
  \end{cases}
  \label{eq:sindromes-alt-sistema}
\end{equation}
De este sistema desconocemos tanto los valores de los \(X_j\) como los de los \(E_j\), pero es que además no es lineal para los \(X_j\).
Como no podemos resolverlo directamente vamos a tratar de encontrar otra forma con la que calcular los valores \(X_j\) y utilizarlos para resolver el sistema lineal que forman los \(E_j\).
Para ello vamos a buscar un sistema lineal que dependa de otras variables \(\sigma_1, \dots, \sigma_v\) que nos conduzca a los valores \(X_j\).
Definimos el \textit{polinomio localizador de errores} \(\sigma(x)\) como
\[
  \sigma(x) = (1 - xX_1)(1 - xX_2) \dots (1 - xX_v) = 1 + \sum_{i=1}^v \sigma_ix^i.
\]
Como vemos inmediatamente por su definición, las raíces de \(\sigma(x)\) son los inversos de los números de coordenadas de error.
Por tanto,
\[
  \sigma(X_j^{-1}) = 1 + \sigma_1X_j^{-1} + \sigma_2X_j^{-2} + \dots + \sigma_vX_j^{-v} = 0
\]
para \(1 \leq j \leq v\).
Si multiplicamos a ambos lados de la expresión por \(E_jX_j^{i+v}\) obtenemos
\[
  E_jX_j^{i+v} + \sigma_1E_jX_j^{i+v-1} + \dots + \sigma_vE_jX_j^{i} = 0
\]
para todo \(i\).
Si sumamos para todo \(j\) en \(1 \leq j \leq v\) tenemos
\[
  \sum_{j=1}^v E_jX_j^{i+v} + \sigma_1\sum_{j=1}^v E_jX_j^{i+v-1} + \dots + \sigma_v \sum_{j=1}^v E_jX_j^i = 0.
\]
Lo que hemos obtenido en estas sumas son los síndromes descritos en (\ref{eq:sindromes-alt}), ya que \(1 \leq i \) y \(i+v \leq 2t\).
Como \(v \leq t\) la expresión anterior se convierte en
\[
  S_{i+v} + \sigma_1S_{i+v-1} + \sigma_2S_{i+v-2} + \dots + \sigma_vS_i = 0,
\]
que equivale a
  \[
    \sigma_1S_{i+v-1} + \sigma_2S_{i+v-2} + \dots + \sigma_vS_i = -S_{i+v},
  \]
para todo \(1 \leq i \leq v\).
Por tanto podemos encontrar los \(\sigma_k\) si resolvemos el sistema de ecuaciones dado por:
\[
  \begin{pmatrix}
    S_1 & S_2 & S_3 & \dots & S_{v-1} & S_v \\
    S_1 & S_2 & S_3 & \dots & S_{v-1} & S_v \\
    S_1 & S_2 & S_3 & \dots & S_{v-1} & S_v \\
     & & & \vdots & & \\
    S_1 & S_2 & S_3 & \dots & S_{v-1} & S_v \\
  \end{pmatrix} \begin{pmatrix}
    \sigma_v \\
    \sigma_{v-1} \\
    \sigma_{v-2} \\
    \vdots\\
    \sigma_1
  \end{pmatrix} = \begin{pmatrix}
    -S_{v+1}\\
    -S_{v+2}\\
    -S_{v+3}\\
    \vdots\\
    -S_{2v}
  \end{pmatrix}.
\]
La dificultad de este paso es que desconocemos el valor de \(v\) (el número de errores), por lo que vamos a realizar un procedimiento iterativo.
Suponemos que nuestro número de errores es \(\mu = t\), que es el máximo que podemos corregir.
Tenemos que quedarnos con el menor valor de \(v\) que sea posible.
Para ello tenemos en cuenta que la matriz
\[
  M_{\mu} = \begin{pmatrix}
    S_1 & S_2 & \dots & S_{\mu} \\
    S_2 & S_3 & \dots & S_{\mu + 1}\\
     & & \vdots & \\
    S_{\mu} & S_{\mu+1} & \dots & S_{2\mu - 1}
  \end{pmatrix}
\] será no singular si \(\mu = v\) y singular si \(\mu > v\) \parencite[Lema 5.4.2]{huffman_fundamentals_2003}.
% MAYBE: copiar demostración de esto????
Así, si \(M_{\mu}\) es singular reducimos el valor de \(\mu\) en 1, \(\mu = \mu - 1\) y probamos de nuevo si \(M_{\mu}\) es singular.
Repetimos hasta encontrar una matriz que no sea singular.
Ese valor \(\mu\) será el número de errores \(v\).
Conocido el tamaño podemos resolver el sistema y obtener los valores \(\sigma_k\).
Ahora solo tenemos que deshacer el camino que hemos recorrido hasta ahora.
Conocidos los \(\sigma_k\) podemos determinar \(\sigma(x)\) y con él, sus raíces, utilizando el procedimiento que queramos, usualmente, calculando reiteradamente \(\sigma(\alpha^i)\) para \(0 \leq i < n\) hasta encontrarlas.
Como ya dijimos, si las invertimos hallaremos los valores de \(X_j\), y con ellos ya podemos resolver el sistema (\ref{eq:sindromes-alt-sistema}), obteniendo así los valores de los \(E_j\).
Conocidos todos los valores de \(X_j\) y \(E_j\) podemos obtener los de \(k_j\) y \(e_{k_j}\), con los que podemos determinar el vector de error \(e(x)\).
Ya solo queda restar \(y(x) - e(x)\) para obtener el mensaje original.

En resumen, el algoritmo consiste en: \begin{enumerate}
  \item Determinar los síndromes del mensaje recibido.
  \item Encontrar el polinomio localizador.
  \item Hallar las raíces del polinomio localizador e invertirlas para obtener las coordenadas de error \(k_j\).
  \item Utilizar estos inversos para resolver el sistema de ecuaciones formado por los síndromes, obteniendo así las magnitudes de error \(e_{k_j}\).
  \item Hallar el vector de error \(e(x)\) y restárselo al mensaje \(y(x)\).
\end{enumerate}

Hemos expresado en el algoritmo \ref{alg:pgz-cc} el algoritmo PGZ en pseudocódigo siguiendo este esquema.
A partir de él se ha realizado una implementación en el sistema Sage que puede consultarse en los archivos enlazadas en el anexo \ref{annex:pgz-sage}.
Veamos un par de ejemplos que utilizan esta implementación.

\begin{example}
  Sea \(\mathcal C\) un \([15, 7]\) código BCH en sentido estricto de distancia designada \(\delta = 5\).
  Supongamos que recibimos el mensaje \(y(x) = x^{10} + x^9 + x^6 + x^5 + x + 1\).
  Vamos a corregir los errores que puedan haberse producido en la transmisión.
  \begin{lstlisting}[gobble=4]
    sage: C = codes.BCHCode(GF(2), 15, 5, offset=1)
    sage: x = polygen(GF(2))
    sage: y = 1 + x +x^5 + x^6 + x^9 + x^10
    sage: DEBUG = true
    sage: PGZ(C, y)
    > polinomio generador: x^8 + x^7 + x^6 + x^4 + 1
      raíz primitiva: z4
      síndromes: [z4^2, z4 + 1, z4^3 + z4^2 + z4, z4^2 + 1]
      tamaño de m_mu: 2
      matriz m_mu: 
      [            z4^2           z4 + 1]
      [          z4 + 1 z4^3 + z4^2 + z4]
      vector b_mu: (z4^3 + z4^2 + z4, z4^2 + 1)
      matriz de soluciones de m_mu*S = b_mu: (z4^3 + 1, z4^2)
      polinomio localizador sigma(x): (z4^3 + 1)*x^2 + z4^2*x + 1
      raíces de sigma(x): [(z4^2 + z4, 1), (z4^3 + z4^2 + z4, 1)]
      X_j: [z4^2 + z4 + 1, z4 + 1]
      k_j: [10, 4]
      error e: x^10 + x^4
    > x^9 + x^6 + x^5 + x^4 + x + 1
  \end{lstlisting}
  El mensaje corregido es por tanto \(m(x) = x^9 + x^6 + x^5 + x^4 + x + 1\).
\end{example}

\begin{example}
  Sea \(\mathcal C\) un \([15, 5]\) código BCH en sentido estricto de distancia designada \(\delta = 5\).
  Supongamos que recibimos el mensaje \(y(x) = x^{12} + x^9 + x^7 + x^5 + x^4 + x + 1\).
  Vamos a corregir los errores que puedan haberse producido en la transmisión.
  \begin{lstlisting}[gobble=4, breaklines=false, basicstyle=\small\ttfamily]
    sage: C = codes.BCHCode(GF(2), 15, 7, offset=1)
    sage: x = polygen(GF(2))
    sage: y = 1 + x +x^4 + x^5 + x^7 + x^9 + x^12
    sage: DEBUG = true
    sage: PGZ(C, y)
    > polinomio generador: x^10 + x^8 + x^5 + x^4 + x^2 + x + 1
      raíz primitiva: z4
      síndromes: [z4^3, z4^3 + z4^2, z4^3, z4^3 + z4^2 + z4 + 1, 0, z4^3 + z4^2]
      tamaño de m_mu: 3
      matriz m_mu: 
      [                z4^3          z4^3 + z4^2                 z4^3]
      [         z4^3 + z4^2                 z4^3 z4^3 + z4^2 + z4 + 1]
      [                z4^3 z4^3 + z4^2 + z4 + 1                    0]
      vector b_mu: (z4^3 + z4^2 + z4 + 1, 0, z4^3 + z4^2)
      matriz de soluciones de m_mu*S = b_mu: (z4^3 + z4^2, z4^2 + 1, z4^3)
      polinomio localizador sigma(x): (z4^3 + z4^2)*x^3 + (z4^2 + 1)*x^2 + z4^3*x + 1
      raíces de sigma(x): [(1, 1), (z4 + 1, 1), (z4^2 + z4, 1)]
      X_j: [1, z4^3 + z4^2 + z4, z4^2 + z4 + 1]
      k_j: [0, 11, 10]
      error e: x^11 + x^10 + 1
    > x^12 + x^11 + x^10 + x^9 + x^7 + x^5 + x^4 + x
  \end{lstlisting}
  El mensaje corregido es por tanto \(m(x) = x^{12} + x^{11} + x^{10} + x^9 + x^7 + x^5 + x^4 + x\).
\end{example}

\begin{Ualgorithm}[htbp]
  \DontPrintSemicolon
  \KwIn{el código \(\mathcal C\), el mensaje recibido \(y(x)\)}
  \KwOut{el mensaje decodificado \(c(x)\)}
  \(\delta \longleftarrow\) distancia designada de \(\mathcal C\)\;
  \(t \longleftarrow \lfloor(\delta - 1)/2\rfloor\)\;
  \(g \longleftarrow\) polinomio generador de \(\mathcal C\)\;
  \(\alpha \longleftarrow\) raíz primitiva del cuerpo de descomposición usado para generar el conjunto característico de \(\mathcal C\)\;
  \tcp{Paso 1: calcular síndromes}
  \For{\(1 \leq i \leq 2t\)}{
      $S_i \longleftarrow y(\alpha^i)$\;
  }
  \tcp{Paso 2: hallar polinomio localizador}
  \(\mu \longleftarrow t\)\;
  \(M_{\mu} \longleftarrow (S_{i + j - 1})_{i, j}\footnotemark\,, 1 \leq i, j \leq \mu \)\;
  \While{\(M_{\mu}\) es no singular}{
  \(\mu \longleftarrow \mu - 1\)\;
  \(M_{\mu} \longleftarrow (S_{i + j - 1})_{i, j}\,, 1 \leq i, j \leq \mu \)\;
  }
  \(v \longleftarrow \mu\)\;
  \(\sigma \longleftarrow (\sigma_{v - i + 1})_i, 1 \leq i \leq v\)\;
  \(b_{\mu} \longleftarrow (-S_{v + i})_i, 1 \leq i \leq v\)\;
  \(\sigma_k \longleftarrow\) soluciones del sistema \(M_{\mu}\sigma = b_{\mu}\)\;
  \(\sigma(x) \longleftarrow 1 + \sum_{i=1}^{v} \sigma_i x^i\)\;
  \tcp{Paso 3: obtener las coordenadas de error}
  \(r_k \longleftarrow\) raíces de \(\sigma(x)\)\;
  \(X_j \longleftarrow r_j^{-1}\) \;
  \(k_j \longleftarrow \log_{\alpha}(X_j)\) \;
  \tcp{Paso 4: obtener las magnitudes de error}
  \(M_{S} \longleftarrow (X_{j}^{i})_{i, j}\,, 1 \leq i, j \leq v \)\;
  \(E \longleftarrow (E_i)_i, 1 \leq i \leq v\)\;
  \(b_{S} \longleftarrow (S_{i})_i, 1 \leq i \leq v\)\;
  \(E_k \longleftarrow\) soluciones del sistema \(M_{S}E = b_{S}\)\;
  \tcp{Paso 5: calcular el mensaje original}
  \(e(x) \longleftarrow \sum_{j=1}^v E_ix^{k_i}\)\;
  \(c(x) = y(x) - e(x)\)\;
  \caption{Peterson-Gorenstein-Zierler para códigos cíclicos.}
  \label{alg:pgz-cc}
\end{Ualgorithm}
\footnotetext{Aquí estamos describiendo una matriz por sus entradas, las filas varían en \(i\), y las columnas, en \(j\).}