\chapter{Algoritmo de Peterson-Gorenstein-Zierler para códigos cíclicos sesgados}

En este capítulo nos adentramos finalmente en el algoritmo que es el objeto de nuestro estudio, el algoritmo de Peterson-Gorenstein-Zierler para códigos cíclicos sesgados.
La fuente de este capítulo es \parencite{gomez-torrecillas_petersongorensteinzierler_2018}.

En lo que sigue vamos a considerar que \(\mathcal C\) es un código RS sesgado en sentido estricto, es decir, que bajo la definición que dimos en el capítulo anterior, el valor \(r = 0\).
% TODO: no acabo de entender todo esto de la notación \alpha' porque no vuelve a mencionarse
También, con el objetivo de simplificar un poco la notación, escribiremos \(\alpha' = \sigma^r(\alpha)\), lo que nos proporcionará una base normal.
% TODO: tengo que explicar lo que es una base normal
Continuando con esta notación tenemos que \(\sigma^r(\beta) = \beta' = (\alpha')^{-1}\sigma(\alpha')\) por lo que el polinomio \([x - \beta', \dots, x - \sigma^{\delta - 2}(\beta)]_i\) será un generador del código \(\mathcal C\).
El ideal por la izquierda \(\mathfrak v^{-1}(\mathcal C)\) está generado por \([x - \beta, x - \sigma(\beta), \dots, x - \sigma^{\delta - 2}(\beta')]_i\) para algún \(2 \leq \delta \leq n\).
La distancia del código \(\mathcal C\) es \(\delta\) por el teorema \ref{th:distancia-skew-rs} y demostraremos más adelante que el algoritmo que vamos a describir permite corregir hasta \(t = \lfloor (\delta - 1)/2 \rfloor\) errores.

Nos encontramos en la misma situación que en casos anteriores.
Se quiere enviar un mensaje \(\mathbf{m} = (m_0, \dots, m_{n - \delta})\) a través de un canal.
Con la identificación de \(\mathcal C\) y \(\mathfrak v^{-1}(\mathcal C)\)podemos escribir \(\mathbf{m}\) como un polinomio, de forma que tenemos que \(m = \sum_{i=0}^{n-\delta}m_ix^{i}\).
Codificamos el mensaje realizando el producto por el generador, \(c = mg\).
Suponemos que recibimos el mensaje \(y = c + e\) donde \(e = e_1x^{k_1} + \dots + e_vx^{k_v}\) con \(v \leq t\) es el error que se ha producido durante la transmisión.
Para determinarlo es necesario por tanto conocer tanto las magnitudes de error \((e_{1}, \dots, e_{v})\) como las coordenadas de error \((k_1, \dots, k_v)\).

Vamos a describir un algoritmo para decodificar códigos cíclicos sesgados similar al ya descrito para códigos BCH.
Así, el primer paso es el del cálculo de los síndromes, que será un procedimiento análogo al realizado entonces pero utilizando la definición de norma \(i\)-ésima de un elemento.
Por tanto para cada \(0 \leq i \leq 2t - 1\) el \emph{síndrome} \(i\)\emph{-ésimo} \(s_i\) del polinomio recibido \(y\) se define como el resto de dividir por la izquierda dicho polinomio \(y\) entre \(x - \sigma^{i}(\beta)\).
Como \(c\) es divisible por la derecha por cada \(x - \sigma^{i}(\beta)\) para \(i = 0, \dots, \delta - 2\) se tiene que 
\begin{align}
  s_i &= \sum_{j = 0}^{n-1}y_jN_j(\sigma^{i}(\beta)) = \sum_{j=1}^{v}e_jN_{k_j}(\sigma^{i}(\beta))\nonumber\\
   &= \sum_{j = 1}^{v}e_j\sigma^{i}(\alpha^{-1})\sigma^{i+k_j}(\alpha) = \sigma^{i}(\alpha^{-1})\sum_{j = 1}^{v}e_j\sigma^{i+k_j}(\alpha).
   \label{eq:sindromes-sesgados}
\end{align}

\begin{proposition}
  \label{prop:pgz-sesgados-magnitudes-error}
  Las magnitudes de error \((e_{1}, \dots, e_{v})\) son las soluciones del sistema de ecuaciones lineales
  \[
    X \begin{pmatrix}
     \sigma^{k_1}(\alpha) & \sigma^{k_1 + 1}(\alpha) & \dots & \sigma^{k_1 + v - 1}(\alpha)\\ 
     \sigma^{k_2}(\alpha) & \sigma^{k_2 + 1}(\alpha) & \dots & \sigma^{k_2 + v - 1}(\alpha)\\ 
     \vdots & \vdots & \ddots & \vdots\\ 
     \sigma^{k_v}(\alpha) & \sigma^{k_v + 1}(\alpha) & \dots & \sigma^{k_v + v - 1}(\alpha)\\ 
    \end{pmatrix}
    = (\alpha s_0, \sigma(\alpha)s_1, \dots, \sigma^{v-1}(\alpha)s_{v-1}).
  \]
\end{proposition}

\begin{proof}
  Comenzamos viendo que
  \begin{align*}
    &\phantom{={}} \begin{pmatrix}
      \sigma^{k_1}(\alpha) & \sigma^{k_1 + 1}(\alpha) & \dots & \sigma^{k_1 + v - 1}(\alpha)\\ 
      \sigma^{k_2}(\alpha) & \sigma^{k_2 + 1}(\alpha) & \dots & \sigma^{k_2 + v - 1}(\alpha)\\ 
      \vdots & \vdots & \ddots & \vdots\\ 
      \sigma^{k_v}(\alpha) & \sigma^{k_v + 1}(\alpha) & \dots & \sigma^{k_v + v - 1}(\alpha)\\ 
     \end{pmatrix}\\
     &= \begin{pmatrix}
      \sigma^{k_1}(\alpha) & \sigma\sigma^{k_1}(\alpha) & \dots & \sigma^{v - 1}\sigma^{k_1}(\alpha)\\ 
      \sigma^{k_2}(\alpha) & \sigma\sigma^{k_2}(\alpha) & \dots & \sigma^{v - 1}\sigma^{k_2}(\alpha)\\ 
      \vdots & \vdots & \ddots & \vdots\\ 
      \sigma^{k_v}(\alpha) & \sigma\sigma^{k_v}(\alpha) & \dots & \sigma^{v - 1}\sigma^{k_v}(\alpha)\\ 
     \end{pmatrix},
  \end{align*}
  que por \parencite[Lema 2.1]{gomez-torrecillas_petersongorensteinzierler_2018} es no singular.
  Por (\ref{eq:sindromes-sesgados}) tenemos que 
  \[
    \sigma^{i}(\alpha)s_i = \sum_{j = 1}^{v}e_j\sigma^{i + k_j}(\alpha),
  \]
  por lo que es evidente que dado \(X = (e_1, \dots, e_v)\) tenemos que
  \[
    (e_1, \dots, e_v) \begin{pmatrix}
      \sigma^{k_1}(\alpha) & \sigma^{k_1 + 1}(\alpha) & \dots & \sigma^{k_1 + v - 1}(\alpha)\\ 
      \sigma^{k_2}(\alpha) & \sigma^{k_2 + 1}(\alpha) & \dots & \sigma^{k_2 + v - 1}(\alpha)\\ 
      \vdots & \vdots & \ddots & \vdots\\ 
      \sigma^{k_v}(\alpha) & \sigma^{k_v + 1}(\alpha) & \dots & \sigma^{k_v + v - 1}(\alpha)\\ 
     \end{pmatrix} = \left(\sigma^{i}(\alpha)s_i\right)_{1 \times v},
  \]
  como queríamos demostrar.
\end{proof}

Como consecuencia de la proposición \ref{prop:pgz-sesgados-magnitudes-error} el proceso de decodificación se reduce a encontrar las coordenadas de error \(\{k_1, \dots, k_v\}\).
Para ello al igual que hicimos en el caso del algoritmo para códigos BCH vamos a definir un polinomio localizador de errores, que en este caso vendrá dado por 
\[
  \lambda = \left[x - \sigma^{k_1}(\beta), x - \sigma^{k_2}(\beta), \dots, x - \sigma^{k_v}(\beta)\right]_{i}.
\]
Por el lema \ref{lem:pol-t-beta} este polinomio \(\lambda\) tiene grado \(v\) y sus raíces nos permitirán determinar las coordenadas de error.

Observamos que, por definición, un polinomio 
\[
  f = \sum_{k = 0}^{n-1}f_k x^{k} \in \mathcal R\lambda \;\text{ si y solo si }\; x - \sigma^{k_j}(\beta) \mid_d f \quad\text{para todo } j = 1, \dots, v,
\]
o bien si y solo si
\[
  \sum_{k=0}^{n -1}f_k N_k(\sigma^{k_j}(\beta))= 0 \quad\text{para todo } j = 1, \dots, v.
\]
De esta forma podemos expresar la condición \((f_0, \dots, f_{n-1}) \in \mathfrak v(\mathcal R\lambda)\) en forma de ecuación matricial, pues si \((f_0, \dots, f_{n-1}) \in \mathfrak v(\mathcal R\lambda)\) se verifica que \((f_0, \dots, f_n)T = 0\), donde
\[
  T = \begin{pmatrix}
    N_{0}(\sigma^{k_1}(\beta)) & N_{0}(\sigma^{k_2}(\beta)) & \dots & N_{0}(\sigma^{k_v}(\beta))\\
    N_{1}(\sigma^{k_1}(\beta)) & N_{1}(\sigma^{k_2}(\beta)) & \dots & N_{1}(\sigma^{k_v}(\beta))\\
    \vdots & \vdots & & \vdots\\
    N_{n-1}(\sigma^{k_1}(\beta)) & N_{n-1}(\sigma^{k_2}(\beta)) & \dots & N_{n-1}(\sigma^{k_v}(\beta))\\
  \end{pmatrix}.
\]
Esto, junto a que por (\ref{eq:norma-beta}) podemos expresar las normas anteriores como \(N_k(\sigma^{k_j}(\beta)) = \sigma^{k_j}(\alpha)^{-1}\sigma^{k_j + k}(\alpha)\), nos da una forma de obtener todos los elementos de \(\mathcal R\lambda\), pues como deben verificar la ecuación antes comentada, forman el núcleo por la izquierda de la matriz
\[
  \Sigma = \begin{pmatrix}
    \sigma^{k_1}(\alpha) & \sigma^{k_2}(\alpha) & \dots & \sigma^{k_v}(\alpha)\\
    \sigma^{k_1 + 1}(\alpha) & \sigma^{k_2 + 1}(\alpha) & \dots & \sigma^{k_v + 1}(\alpha)\\
    \vdots & \vdots & \ddots & \vdots \\
    \sigma^{k_1 + n - 1}(\alpha) & \sigma^{k_2 + n - 1}(\alpha) & \dots & \sigma^{k_v + n - 1}(\alpha)\\
  \end{pmatrix}
  = \left( \begin{array}{@{}c@{}}
    \Sigma_0\\\hline
    \Sigma_1
  \end{array}\right),
\]
donde \(\Sigma_0\) se corresponde a las primeras \(v + 1\) filas de la matriz \(\Sigma\) anterior.

Consideramos ahora la matriz 
\[
  E = \begin{pmatrix}
    e_1 & \sigma^{-1}(e_1) & \dots & \sigma^{-v + 1}(e_1)\\
    e_2 & \sigma^{-1}(e_2) & \dots & \sigma^{-v + 1}(e_2)\\
    \vdots & \vdots & \ddots & \vdots \\
    e_v & \sigma^{-1}(e_v) & \dots & \sigma^{-v + 1}(e_v)\\
  \end{pmatrix}.
\]
Así podemos considerar la siguiente matriz, que expresaremos por sus entradas para la fila \(k\) y la columna \(i\),
\[
  S = \Sigma E = \left(\sum_{j=1}^v \sigma^{-i}(e_j)\sigma^{k_j+k}(\alpha)\right)_{n \times v}.
\]
Por (\ref{eq:sindromes-sesgados}), cuando \(k + i < 2t - 1\), la componente \((k, i)\)-ésima puede escribirse como \(\sigma^{-i}(s_{k+i})\sigma(\alpha)\), de tal forma que podemos dividir la matriz \(S\) como
\[
  S = \left( \begin{array}{@{}c@{}}
    S_0\\\hline
    S_1
  \end{array}\right),
\]
donde \(S_0\) viene dada por
\[
  S_0 = \begin{pmatrix}
    s_0\alpha & \sigma^{-1}(s_1)\alpha & \dots & \sigma^{-v+1}(s_{v-1})\alpha\\
    s_1\sigma(\alpha) & \sigma^{-1}(s_1)\sigma(\alpha) & \dots & \sigma^{-v+1}(s_{v})\sigma(\alpha)\\
    \vdots & \vdots & & \vdots \\
    s_v\sigma^v(\alpha) & \sigma^{-1}(s_{v+1})\sigma^v(\alpha) & \dots & \sigma^{-v+1}(s_{2v-1})\sigma^v(\alpha)\\
  \end{pmatrix}_{(v + 1) \times v}
\]
y cuyos coeficientes pueden calcularse directamente a partir del polinomio \(y\).
Para calcular el parámetro \(v\) utilizaremos el mismo procedimiento utilizado en el algoritmo PGZ para códigos BCH.
Para cualquier \(1 \leq r \leq t\) denotaremos por \(S^r\) a la matriz
\[
  S^r = \begin{pmatrix}
    s_0\alpha & \sigma^{-1}(s_1)\alpha & \dots & \sigma^{-r+1}(s_{r-1})\alpha\\
    s_1\sigma(\alpha) & \sigma^{-1}(s_1)\sigma(\alpha) & \dots & \sigma^{-r+1}(s_{r})\sigma(\alpha)\\
    \vdots & \vdots & & \vdots \\
    s_t\sigma^t(\alpha) & \sigma^{-1}(s_{t+1})\sigma^t(\alpha) & \dots & \sigma^{-r+1}(s_{t+r-1})\sigma^t(\alpha)\\
  \end{pmatrix}_{(t + 1) \times r}.
\]
Igual que antes, tenemos que para todo \(r \leq t\) se tiene que \(S^r = \Sigma^tE^r\), donde 
\[
  E^r = \begin{pmatrix}
    e_1 & \sigma^{-1}(e_1) & \dots & \sigma^{-r + 1}(e_1)\\
    e_2 & \sigma^{-1}(e_2) & \dots & \sigma^{-r + 1}(e_2)\\
    \vdots & \vdots & \ddots & \vdots \\
    e_v & \sigma^{-1}(e_v) & \dots & \sigma^{-r + 1}(e_v)\\
  \end{pmatrix}_{v \times r}.
\]
y
\[
  \Sigma^t = \begin{pmatrix}
    \sigma^{k_1}(\alpha) & \sigma^{k_2}(\alpha) & \dots & \sigma^{k_v}(\alpha)\\
    \sigma^{k_1 + 1}(\alpha) & \sigma^{k_2 + 1}(\alpha) & \dots & \sigma^{k_v + 1}(\alpha)\\
    \vdots & \vdots & \ddots & \vdots \\
    \sigma^{k_1 + t}(\alpha) & \sigma^{k_2 + t}(\alpha) & \dots & \sigma^{k_v + t}(\alpha)\\
  \end{pmatrix}_{(t+1)\times v}.
\]

\begin{lemma}
  \label{lem:pgz-sesgados-rangos}
  Para cada \(r \leq t\) se tiene que \(\rank(S^r) = \rank(\Sigma E^r) = \rank(E^r)\).
\end{lemma}

\begin{proof}
  Por \parencite[Lema 2.1]{gomez-torrecillas_petersongorensteinzierler_2018} tiene que \(\rank(\Sigma) = \rank(\Sigma^t) = v\).
  Usando la desigualdad del rango de Sylvester se tiene que
  % MAYBE: citar?
  \[
    \min\{\rank(\Sigma), \rank(E^r)\} \geq \rank(\Sigma E^r) \geq \rank(\Sigma) + \rank E^r - v = \rank(E^r).
  \]
  Por tanto \(\rank(\Sigma E^r) = \rank(E^r)\).
  Con un razonamiento análogo se puede comprobar que \(\rank(S^r) = \rank(E^r)\).
\end{proof}

Hemos de que calcular el mayor natural \(r\) tal que la matriz \(S^r\) tenga rango máximo.
Por el lema \ref{lem:pgz-sesgados-rangos} que acabamos de demostrar es también el mayor natural \(r \leq t\) tal que las matrices \(E^r\) y \(\Sigma E^r\) tienen rango máximo.
Denotaremos por \(\mu\) a tal máximo.

\begin{lemma}
  \label{lem:pgz-sesgados-rango-mu}
  Para cada \(r\) tal que \(\mu \leq r \leq t\) se tiene que \(\rank(E^r) = \rank(S^r) = \mu\).
  Por tanto, \(\mu \leq v\).
\end{lemma}

\begin{proof}
  Por el lema \ref{lem:pgz-sesgados-rangos} \(\mu = \rank(E^{\mu}) = \rank(S^{\mu})\), por lo que suponemos que \(\mu < r\).
  Por la maximalidad de \(\mu\) tenemos que la \((\mu + 1)\)-ésima columna de \(E^r\) es una combinación lineal de las \(\mu\) columnas anteriores.
  Aplicando \(\sigma^{-1}\) obtenemos que la \((\mu + 2)\)-ésima columna es una combinación lineal de las columnas segunda a \(\mu + 1\)-ésima, y por tanto una combinación lineal de las primeras \(\mu\) columnas.
  Si repetimos el proceso obtenemos que todas las columnas desde la \((\mu + 1)\)-ésima hasta la \(r\)-ésima son combinaciones lineales de las primeras \(\mu\) columnas, y por tanto \(\rank(E^r) = \mu\).
  Como \(E^r\) tiene \(v\) filas, \(\mu \leq v\).
  Finalmente, \(\rank(S^r) = \mu\) de nuevo por el lema \ref{lem:pgz-sesgados-rangos}.
\end{proof}

\begin{proposition}
  \label{prop:pgz-sesgados-kernel-sesgado}
  El núcleo por la izquierda \(V\) de la matriz \(\Sigma E^{\mu}\) es un código cíclico sesgado.
  Por tanto se tiene que \(\mathfrak v^{-1}(V) = \mathcal R\rho\) para algún polinomio \(\rho \in \mathcal R\) de grado \(\mu\).
  Se tiene además que \(\rho\) es un divisor por la derecha de \(\lambda\).
\end{proposition}

\begin{proof}
  Demostraremos la primera afirmación comprobando que si el vector \((a_0, \dots, a_{n-2}, a_{n-1}) \in V \subseteq \mathbb F_q^n\) también se da que el desplazamiento cíclico del vector anterior \((\sigma(a_{n-1}), \sigma(a_0), \dots, \sigma(a_{n-2})) \in V\).
  Recordemos que el desplazamiento tiene esta forma porque \(xa_ix^{i} = \sigma(a_i)x^{i+1}\) para cada \(0 \leq i \leq n -1\).
  Supongamos entonces que \((a_0, a_1, \dots, a_{n-1})\Sigma E^{\mu} = 0\).
  La maximalidad de \(\mu\) nos asegura que la última columna de \(E^{\mu + 1}\) es una combinación lineal de las \(\mu\) columnas anteriores.
  Por tanto, \((a_0, a_1, \dots, a_{n-1})\Sigma E^{\mu + 1} = 0\).
  Así,
  \begin{align*}
    0 &= (a_0, a_1, \dots, a_{n-1})\Sigma E^{\mu + 1}\\
      &= (a_0, a_1, \dots, a_{n-1})\left( \begin{array}{@{}c|c@{}}
        0 & I_{n-1}\\\hline
        1 & 0
      \end{array}\right)\left( \begin{array}{@{}c|c@{}}
        0 & 1\\\hline
        I_{n-1} & 0
      \end{array}\right)\Sigma E^{\mu + 1}\\
      &= (a_{n-1}, a_0, \dots, a_{n-2})\left( \begin{array}{@{}c|c@{}}
        0 & 1\\\hline
        I_{n-1} & 0
      \end{array}\right)\Sigma E^{\mu + 1}.
  \end{align*}
  Si aplicamos \(\sigma\) a esta ecuación matricial componente a componente obtenemos
  \[
    (\sigma(a_{n-1}), \sigma(a_0), \dots, \sigma(a_{n-2}))\left( \begin{array}{@{}c|c@{}}
      0 & 1\\\hline
      I_{n-1} & 0
    \end{array}\right)\Sigma \sigma(E)\sigma(E^{\mu}) = 0.
  \]
  Observamos que
  \[
    \Sigma = \left( \begin{array}{@{}c|c@{}}
      0 & 1\\\hline
      I_{n-1} & 0
    \end{array}\right)\sigma(\Sigma) \quad\text{y}\quad \sigma(E^{\mu + 1}) = \left( \begin{array}{@{}c|c@{}}
      \sigma(e_1) & \multirow{3}{*}{\(E^{\mu}\)}\\
      \vdots & \\
      \sigma(e_v) & \\
    \end{array}\right)
  \]
  por lo que 
  \[
    (\sigma(a_{n-1}), \sigma(a_0), \dots, \sigma(a_{n-2}))\Sigma \left( \begin{array}{@{}c|c@{}}
      \sigma(e_1) & \multirow{3}{*}{\(E^{\mu}\)}\\
      \vdots & \\
      \sigma(e_v) & \\
    \end{array}\right) = 0.
  \]
  En particular, \((\sigma(a_{n-1}), \sigma(a_0), \dots, \sigma(a_{n-2}))\Sigma E^{\mu} = 0\) por lo que el desplazamiento cíclico \((\sigma(a_{n-1}), \sigma(a_0), \dots, \sigma(a_{n-2})) \in V\), como queríamos.
  Además, dado que cualquier ideal de \(\mathcal R\) es principal, \(\mathfrak v^{-1}(V)\) lo es y estará generado por un polinomio \(\rho \in \mathcal R\).
  Como \(\mathfrak v(\mathcal R \lambda)\) es el núcleo por la izquierda de la matriz \(\Sigma\) se tiene que \(\mathcal R\lambda \subseteq \mathcal R\rho\), y por tanto \(\rho\) divide por la derecha a \(\lambda\).
  % TODO: explicar esto de encima mejor, no lo entiendo bien
  Finalmente la dimensión de \(\mathcal \rho\) como un \(\mathbb F_q\) espacio vectorial es \(n - \deg(\rho)\).
  Por el lema \ref{lem:pgz-sesgados-rangos} se tiene que \(\rank(\Sigma E^{\mu}) = \mu\) y por tanto \(\deg(\rho) = \mu\).
\end{proof}

\begin{lemma}
  \label{lem:pgz-sesgados-escalonada-st}
  La forma escalonada por columnas de la matriz \(S^t\) es
  \[
    \operatorname{mepc}(S^t) = \left( \begin{array}{@{}c|c@{}}
      I_{\mu} & \multirow{3}{*}{\(0_{(t+ 1)\times (t - \mu)}\)} \\\cline{1-1}
      a_0 \cdots a_{\mu -1 } & \\\cline{1-1}
      H' &
    \end{array}\right),
  \]
  donde \(I_{\mu}\) es la matriz identidad \(\mu \times \mu\) y \(a_0, \dots, a_{\mu - 1} \in \mathbb F_q\) tales que \(\rho = x^{\mu} - \sum_{i = 0}^{\mu - 1}a_ix^{i}\).
\end{lemma}

\begin{proof}
  Por el lema \ref{lem:pgz-sesgados-rango-mu} el \(\rank(S^t) = \mu = \rank(S^{\mu})\), por lo que 
  \[
    \operatorname{mrpc}(S^t) = \left( \begin{array}{@{}c|c@{}}
      \operatorname{mrpc}(S^{\mu}) & 0_{(t + 1) \times (t- \mu)}
    \end{array}\right).
  \]
  La matriz \(S^{\mu}\) consiste en las primeras \(t + 1\) filas de \(\Sigma E^{\mu}\) y ambas tienen el mismo rango \(\mu\), por lo que \(\operatorname{mrpc}(S^{\mu})\) está formada por las primeras \(t + 1\) filas de \(\operatorname{mrpc}(\Sigma E^{\mu})\).
  Por la proposición \ref{prop:pgz-sesgados-kernel-sesgado} \(\mathfrak v(\mathcal R\rho)\) es el núcleo por la izquierda de la matriz \(\operatorname{mrpc}(\Sigma E^{\mu})\).
  Una solución no nula del sistema homogéneo
  \begin{equation}
    \label{eq:pgz-sesgados-mepc-sigma-e-mu}
    X \left( \begin{array}{@{}c|c@{}}
      \multirow{2}{*}{\(\operatorname{mepc}(\Sigma E^{\mu})\)} & 0\\\cline{2-2}
       & I_{n - (\mu + 1)} 
    \end{array}\right) = 0
  \end{equation}
  es un elemento distinto de cero de \(\mathfrak v(\mathcal R(\rho))\) cuyas últimas \(n - (\mu + 1)\) coordenadas son cero.
  Como \(\rho\) tiene grado \(\mu\) y su grado es mínimo en \(\mathcal R \rho\) se deduce que \(\mathfrak v(\rho)\) es la única solución, salvo producto por escalares de (\ref{eq:pgz-sesgados-mepc-sigma-e-mu}).
  Sea \(S_{0}^{\mu}\) la matriz formada por las primeras \(\mu + 1\) filas de \(S^{\mu}\).
  Entonces
  \[
    \operatorname{mepc}(S^{\mu}) = \left( \begin{array}{@{}c@{}}
      \operatorname{mepc}(S_0^{\mu})\\\hline
      H'
    \end{array}\right).
  \]
  Si realizamos más reducciones de columnas utilizando la matriz identidad en el bloque derecho de la matriz (\ref{eq:pgz-sesgados-mepc-sigma-e-mu}) podemos ver que \(\rho\) es también la solución no nula, salvo producto por escalares, del sistema homogéneo
  \begin{equation}
    \label{eq:pgz-sesgados-mepc-sigma-e-mu-4}
    X \left( \begin{array}{@{}c|c@{}}
      \operatorname{mepc}(S_{0}^{\mu}) & 0\\\hline
      0 & I_{n - (\mu + 1)}\\
    \end{array}\right) = 0.
  \end{equation}
  El tamaño de \(\operatorname{mepc}(S_{0}^{\mu})\) es \((\mu + 1) \times \mu\).
  De hecho \(\rank(\operatorname{mepc}(S_{0}^{\mu})) = \mu\) porque el espacio de soluciones de (\ref{eq:pgz-sesgados-mepc-sigma-e-mu-4}) tiene dimensión 1.
  Por tanto solo hay una fila de \(\operatorname{mepc}(S_{0}^{\mu})\) sin pivote.
  Si no es la última entonces habría un polinomio no nulo de \(\mathcal R \rho\) de grado estrictamente menor que \(\mu\), lo cual es imposible.
  Por tanto,
  \[
    \operatorname{mepc}(S_{0}^{\mu}) = \left( \begin{array}{@{}c@{}}
      I_{\mu} \\\hline
      a_0 \dots a_{\mu - 1}
    \end{array}\right).
  \]
  Finalmente \((-a_0, \dots, -a_{\mu - 1}, 1, 0 \dots, 0)\) es una solución no nula de (\ref{eq:pgz-sesgados-mepc-sigma-e-mu-4}), por lo que \(\rho = x^{\mu}- \sigma_{i = 0}^{\mu - 1}a_ix^{i}\).
\end{proof}

 \begin{lemma}
  \label{lem:pgz-sesgados-diagrama}
   Si el ideal por la izquierda \(\mathcal R \rho\) se corresponde, mediante \(\mathfrak v\), con el núcleo por la izquierda de una matriz \(H\) entonces \(H = \Sigma B\) para alguna matriz \(B \in \mathcal M_{v \times \mu}(L)\) que no tiene ninguna fila nula.
 \end{lemma}

\begin{proof}
  % TODO: añadir? referenciar?
\end{proof}

Hay una conexión entre los polinomios \(\rho\) y \(\lambda\).

\begin{proposition}
  \label{prop:pgz-sesgados-lambda-b-descompone-multiplo-rho}
  Sea \(\lambda' \in \mathcal R\) un polinomio que \(\beta\)-descompone totalmente y es múltiplo de \(\rho\).
  Entonces, \(\lambda \mid_d \lambda'\).
\end{proposition}

\begin{proof}
  Por la proposición \ref{prop:pgz-sesgados-kernel-sesgado} se tiene que \(\rho \mid_d \lambda\) y, por hipótesis, \(\rho \mid_d \lambda'\).
  Así, \(\rho \mid_d (\lambda, \lambda')_{r}\).
  De hecho, por el lema \ref{lem:b-descomposicion-mcm-mcd} el polinomio \((\lambda, \lambda')_{r}\) también \(\beta\)-descompone totalmente.
  Denotemos por \(\phi = (\lambda, \lambda')_{r}\).
  Vamos a demostrar que \(\phi = \lambda\), lo que implica el hecho que queremos demostrar.

  Efectivamente, \(\mathcal R \lambda \subseteq \mathcal R\phi\), por lo que \(\mathcal R\phi\) se corresponde con el núcleo por la izquierda de una matriz \(\Sigma Q\), donde \(Q\) es una matriz de rango máximo.
  De forma análoga se tiene que \(\mathcal R\phi \subseteq \mathcal R\rho\), por lo que existe otra matriz \(Q'\) tal que \(\mathcal R\rho\) es el núcleo por la izquierda de \(\Sigma QQ'\).
  Por el lema \ref{lem:pgz-sesgados-diagrama} se tiene que \(\Sigma QQ' = \Sigma B\), donde \(B\) es una matriz de rango máximo y sin ninguna fila nula.
  Por tanto \(QQ' = B\), porque \(\Sigma\) define una aplicación lineal sobreyectiva y \(Q\) no tiene filas nulas.

  Como \(\phi \mid_r \lambda\) cualquier \(\beta\)-raíz de \(\phi\) tiene que ser también \(\beta\)-raíz de \(\lambda\) por lo que pertenece al conjunto \(\{\sigma^{k_1}(\beta), \dots, \sigma^{k_v}(\beta)\}\).
  Obsérvese que por (\ref{eq:equivalencias-divisor-sigma-b}) se tiene que \(\sigma^{k_j}(\beta)\) es una \(\beta\)-raíz de \(\phi\) si y solo si
  \[
    \rank\left( \begin{array}{@{}c|c@{}}
      \Sigma Q & \sigma^{k_j}(\alpha)^{[\sigma]}
    \end{array}\right) = \rank(\Sigma Q).
  \]
  % FIXME: qué es ese sigma entre corchetes?????????
  Por tanto, por el lema \parencite[Lema 2.3]{gomez-torrecillas_petersongorensteinzierler_2018} que \(\phi\) se pueda \(\beta\)-descomponer totalmente implica que \(\{\sigma^{k_1}(\beta), \dots, \sigma^{k_v}(\beta)\}\) es el conjunto de \(\beta\)-raíces de \(\phi\).
  Por tanto, \(\phi = \lambda\).
\end{proof}

Ya estamos en disposición de calcular el polinomio localizador de errores.
Esto completa el diseño del algoritmo de Peterson-Gorenstein-Zierler para códigos cíclicos sesgados, que puede verse en el algoritmo \ref{alg:pgz-sesgados}.

\begin{Ualgorithm}[htbp]
  \small
  \DontPrintSemicolon
  \KwIn{el código \(\mathcal C\), el mensaje recibido \(y = (y_0, \dots, y_{n-1}) \in \mathbb F_q^n\) con no más de \(t\) errores}
  \KwOut{el error \(e = (e_0, \dots, e_{n-1})\) tal que \(y - e \in \mathcal C\)}
  \tcp{Paso 1: calcular síndromes}
  \For{\(0 \leq i \leq 2t - 1\)}{
      $s_i \longleftarrow \sum_{j=0}^{n-1}y_jN_j(\sigma^i(\beta))$\;
  }
  \If{\(s_i = 0\) para todo \(0 \leq i \leq 2t - 1\)}{\Return{\(0\)}}
  \tcp{Paso 2: hallar polinomio localizador y las coordenadas de error}
  \(S^t \longleftarrow \left(\sigma^{-j}(s_{i+j})\sigma^i(\alpha)\right)_{0 \leq i \leq t, 0 \leq j \leq t -1}\)\;
  Calcular
  \[
    \operatorname{mepc}(S^t) = \left( \begin{array}{@{}c|c@{}}
      I_{\mu} & \multirow{3}{*}{\(0_{(t+ 1)\times (t - \mu)}\)} \\\cline{1-1}
      a_0 \cdots a_{\mu -1 } & \\\cline{1-1}
      H' &
    \end{array}\right)
  \]\vspace*{-1.5em}\;% Reducimos un poco el espacio vertical
  \(\rho = (\rho_0, \dots, \rho_{\mu}) \longleftarrow (-a_0, \dots, -a_{\mu-1}, 1)\) y \(\rho N \longleftarrow (\rho_0, \dots, \rho_{\mu}, 0, \dots, 0)N\)\;\label{algl:pgz-sesgados-rho}
  \(\{k_1, \dots, k_v\} \longleftarrow \) coordenadas igual a cero de \(\rho N\)\;\label{algl:pgz-sesgados-pos-error}
  \If{\(\mu \neq v\)}{
    Calcular \[M_{\rho} \longleftarrow \begin{pmatrix}
      \rho_0 & \rho_1 & \dots & \rho_{\mu} & 0 & \dots & 0\\
      0 & \sigma(\rho_0) & \dots & \sigma(\rho_{\mu - 1}) & \sigma(\rho_{\mu}) & \dots & 0\\
       & & \ddots & & & \ddots & \\
      0 & \dots & 0 & \sigma^{n - \mu - 1}(\rho_0) & \dots & \dots & \sigma^{n - \mu - 1}(\rho_{\mu})
    \end{pmatrix}_{(n - \mu) \times n}\]\vspace*{-1.5em}\;% Reducimos un poco el espacio vertical
    \(N_{\rho} \longleftarrow M_{\rho}N\)\;
    \(H_{\rho} \longleftarrow \operatorname{mepf}(N_{\rho})\)\;
    \(H' \longleftarrow\) la matriz obtenida al eliminar las filas de \(H_{\rho}\) distintas de \(\varepsilon_i\) para algún \(i\)\;\label{algl:pgz-sesgados-h-prima}
    \(\{k_1, \dots, k_v\} \longleftarrow\) las coordenadas de las columnas igual a cero de \(H'\)\;
  }
  \tcp{Paso 3: resolver el sistema de los síndromes, obteniendo las magnitudes de error}
  Encontrar \((x_1, \dots, x_v)\) tal que \((x_1, \dots, x_v)(\Sigma^{v-1})^T = (\alpha s_0, \sigma(\alpha)s_1, \dots, \sigma^{v-1}(\alpha)s_{v-1})\)\;\label{algl:pgz-sesgados-solucion-sistema}
  \tcp{Paso 4: construir el error y devolverlo}
  \Return{\((e_0, \dots, e_{n-1})\) con \(e_i = x_i\) para \(i \in \{k_1, \dots, k_v\}\), cero en otro caso}
  \caption{Peterson-Gorenstein-Zierler para códigos cíclicos sesgados.}
  \label{alg:pgz-sesgados}
\end{Ualgorithm}

\begin{theorem}
  Sea \(\mathbb F_q\) un cuerpo finito, \(\sigma \in \operatorname{Aut}(\mathbb F_q)\) de orden \(n\) y \(\mathbb F_q^{\sigma}\) el subcuerpo invariante del generado por \(\sigma\).
  Sea \(\{\alpha, \sigma(\alpha), \dots, \sigma^{n-1}(\alpha)\}\) una base normal de \(\mathbb F_q\) sobre \(\mathbb F_{q}\) y \(\beta = \alpha^{-1}\sigma(\alpha)\).
  Sean \(\mathcal R = \mathbb F_q[x; \sigma]/(x^n - 1)\), \(g = \left[x - \beta, \dots, x - \sigma^{\delta - 2}(\beta)\right]_{i}\) y \(\mathcal C\) el código RS sesgado tal que \(\mathfrak v^{-1}(\mathcal C) = \mathcal Rg\).
  Entonces el algoritmo descrito en el algoritmo \ref{alg:pgz-sesgados} encuentra correctamente el erro \(e = (e_0, \dots, e_{n-1})\) de cualquier vector recibido si el número de coordenadas distintas de cero de \(e\) es \(v \leq t = \lfloor (\delta - 1)/2 \rfloor\).
\end{theorem}

\begin{proof}
  Tras los ajustes iniciales la línea \ref{algl:pgz-sesgados-rho} calcula un divisor por la izquierda \(\rho = \sum_{i = 0}^{\mu}\) del polinomio localizador de errores \(\lambda\), por la proposición \ref{prop:pgz-sesgados-kernel-sesgado} y el lema \ref{lem:pgz-sesgados-escalonada-st}.
  
  Por (\ref{eq:equivalencias-divisor-sigma-b}) la línea \ref{algl:pgz-sesgados-pos-error} calcula todas las \(\beta\)-raíces de \(\rho\).
  Por el lema \ref{lem:pol-t-beta} \(v = \mu\) si y solo si \(\rho\) puede \(\beta\)-descomponerse totalmente.
  En ese caso, por la proposición \ref{prop:pgz-sesgados-lambda-b-descompone-multiplo-rho} \(\rho = \lambda\).
  
  Si \(v \neq \mu\) como el \(\deg(\rho) = \mu\) las fila de \(M_{\rho}\) generan \(\mathcal R\rho\) como un \(\mathbb F_q\) espacio vectorial y las fila de \(N_{\rho}\) también generan \(\mathcal R\rho\) bajo el cambio de base correspondiente a \(N\).
  Como \(H_{\rho}\) es la forma reducida por columnas de \(M_{\rho}\) entonces sus fijas también son una base de \(\mathcal \rho\) como un \(\mathbb F_q\)- espacio vectorial.
  Por el lema \ref{lem:matriz-mf-base} las filas de \(H'\) generan un \(\mathbb F_q\)-subespacio vectorial \(\mathcal R\lambda'\) para algún polinomio \(\lambda'\) que puede \(\beta\)-descomponerse totalmente.
  Como \(H'\) se obtiene eliminando algunas filas de \(H_{\rho}\) se deduce que \(\rho \mid_d \lambda'\).

  Probamos ahora que \(\lambda' = \lambda\).
  Por la proposición \ref{prop:pgz-sesgados-lambda-b-descompone-multiplo-rho} se tiene que \(\lambda \mid_d \lambda'\).
  Supongamos que \(\lambda \neq \lambda'\).
  Entonces la matriz \(H_{\lambda} = \operatorname{mrpf}(M_{\lambda}N)\) contiene una fila adicional \(\varepsilon_{d}\) que no está en \(H'\).
  %TODO: añadir que hay que aplicar el lema 3.1 (dice Gabriel)
  Como \(\mathcal R\lambda \subseteq \mathcal R\rho\),
  \[
    \rank\left( \begin{array}{@{}c@{}}
      H_{\rho}\\\hline
      \varepsilon_{d}
    \end{array}\right) = \rank (H_{\rho}).
  \]
  Por \parencite[Lema 2.4]{gomez-torrecillas_petersongorensteinzierler_2018} \(\varepsilon_{d}\) es una fila de \(H_{\rho}\), por lo que la línea \ref{algl:pgz-sesgados-h-prima} no la elimina.
  Por tanto \(\varepsilon_{d}\) pertenece a \(H'\), una contradicción.
  Por tanto \(\lambda = \lambda'\) y se calculan las coordenadas de error.
  Por la proposición \ref{prop:pgz-sesgados-magnitudes-error} la línea \ref{algl:pgz-sesgados-solucion-sistema} calcula las magnitudes de error.
\end{proof}

Veamos a continuación algunos ejemplos.

\begin{example}
  Sea \(\mathbb F = \mathbb F_2(a)\) un cuerpo con \(2^{12} = 1024\) elementos, donde se verifica la relación \(a^{12} + a^7 + a^{6} + a^{5} + a^{3} + a + 1 = 0\).
  Consideremos el automorfismo \(\mathbb F \to \mathbb F\) dado por \(\sigma = \tau^{10}\), donde \(\tau\) es el automorfismo de Frobenius, tal que \(\sigma(a) = a^{1024}\).
  El orden de \(\sigma\) es \(6\) por lo que un código cíclico sobre \(\mathbb F\) es un ideal por la izquierda del cociente \(\mathcal R = \mathbb F[x; \sigma]/(x^{6} - 1)\).
  Tomaremos \(\alpha = a\), lo que nos proporciona una base normal de \(\mathbb F\), y \(\beta = \sigma(a)a^{-1} = a^{1023}\).
  Las imágenes de \(\beta\) por las potencias de \(\sigma\) nos da el conjunto \(\{a^{1023}, a^{3327}, a^{3903}, a^{4047}, a^{4083}, a^{4092}\}\).
  Consideremos el código RS sesgado generado por
  \[
    g = \left[x - a^{1023}, x - a^{3327}, x - a^{3903}, x - a^{4047}\right]_i = x^4 + a^{2103}x^3 + a^{687}x^2 + a^{1848} + a^{759}.
  \]
\end{example}